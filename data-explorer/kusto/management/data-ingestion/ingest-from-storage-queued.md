---
title:  Kusto.ingest into command (pull data from storage)
description: This article describes the DM queued ingest command (pull data from storage) in Azure Data Explorer.
ms.reviewer: ???
ms.topic: reference
ms.date: 06/19/2024
---
# Queued ingest from storage

The `.ingest-from-storage-queued into` command queues data from one or more cloud storage files for ingestion into a table.

[!INCLUDE [direct-ingestion-note](../../../includes/direct-ingestion-note.md)]

## Permissions

You must have at least [Table Ingestor](../access-control/role-based-access-control.md) permissions to run this command.

## Syntax

`.ingest-from-storage-queued` `into` `table` [database(*DatabaseName*).]*TableName*

[EnableTracking=EnableTrackingValue]

[SkipBatching=SkipBatchingValue]

[CompressionFactor=CompressionFactorValue]

[with ( *IngestionPropertyName* = *IngestionPropertyValue* [, ...] )]

<|

*SourceDataLocators*

[!INCLUDE [syntax-conventions-note](../../../includes/syntax-conventions-note.md)]

SkipBatching is false
EnableTracking is false

## Parameters

|Name|Type|Required|Description|
|--|--|--|--|
|*DatabaseName*| `string` | |The name of the database into which to ingest data.  If no database name is provided, the database schema of the database in context is used.|
|*TableName*| `string` | :heavy_check_mark:|The name of the table into which to ingest data.|
|*EnableTracking*| `boolean` | | If `true`, the blob ingestion will be tracked so that <span style="background:yellow">TODO</span>. Default is `false`.  |
|*SkipBatching*| `boolean` | | If `true`, the blobs will not be batch (either together or with other blobs) and each blob will be ingested individually. Default is `false`.  |
|*CompressionFactor*| `real` | |Compression factor (ratio) between the original size and the compressed size of blobs.  This is useful when blobs are provided in a compressed format to estimate the original size of the data (for batching purposes). |
|*SourceDataLocators*| `string` | :heavy_check_mark:|One or many (maximum 100) [storage connection strings](../../api/connection-strings/storage-connection-strings.md) separated by a return character.  Each connection string must refer to a single file hosted by a storage account.|

> [!NOTE]
> We recommend using [obfuscated string literals](../../query/scalar-data-types/string.md#obfuscated-string-literals) for the *SourceDataPointer*. The service will scrub credentials in internal traces and error messages.

[!INCLUDE [ingestion-properties](../../../includes/ingestion-properties.md)]

## Authentication and authorization

Each storage connection string indicates the authorization method to use for access to the storage. Depending on the authorization method, the principal may need to be granted permissions on the external storage to perform the ingestion.

The following table lists the supported authentication methods and the permissions needed for ingesting data from external storage.

|Authentication method|Azure Blob Storage / Data Lake Storage Gen2|Data Lake Storage Gen1|
|--|--|--|
|[Impersonation](../../api/connection-strings/storage-authentication-methods.md#impersonation)|Storage Blob Data Reader|Reader|
|[Shared Access (SAS) token](../../api/connection-strings/storage-authentication-methods.md#shared-access-sas-token)|List + Read|This authentication method isn't supported in Gen1.|
|[Microsoft Entra access token](../../api/connection-strings/storage-authentication-methods.md#azure-ad-access-token)||
|[Storage account access key](../../api/connection-strings/storage-authentication-methods.md#storage-account-access-key)||This authentication method isn't supported in Gen1.|
|[Managed identity](../../api/connection-strings/storage-authentication-methods.md#managed-identity)|Storage Blob Data Reader|Reader|

## Returns

The result of the command is a table with as many records
as there are data shards ("extents") generated by the command.
If no data shards have been generated, a single record is returned
with an empty (zero-valued) extent ID.

|Name       |Type      |Description                                                                |
|-----------|----------|---------------------------------------------------------------------------|
|ExtentId   |`guid`    |The unique identifier for the data shard that was generated by the command.|
|ItemLoaded |`string`  |One or more storage files that are related to this record.             |
|Duration   |`timespan`|How long it took to perform ingestion.                                     |
|HasErrors  |`bool`    |Whether this record represents an ingestion failure or not.                |
|OperationId|`guid`    |A unique ID representing the operation. Can be used with the `.show operation` command.|

>[!NOTE]
> This command doesn't modify the schema of the table being ingested into. If necessary, the data is "coerced" into this schema during ingestion, not the other way around (extra columns are ignored, and missing columns are treated as null values).

## Examples

### Azure Blob Storage with shared access signature

The following example instructs your cluster to read two blobs from Azure Blob Storage
as CSV files, and ingest their contents into table `T`. The `...` represents
an Azure Storage shared access signature (SAS) which gives read access to each
blob. Note also the use of obfuscated strings (the `h` in front of the string
values) to ensure that the SAS is never recorded.

```kusto
.ingest into table T (
    h'https://contoso.blob.core.windows.net/container/file1.csv?...',
    h'https://contoso.blob.core.windows.net/container/file2.csv?...'
)
```

### Azure Blob Storage with managed identity

The following example shows how to read a CSV file from Azure Blob Storage and ingest its contents into table `T` using managed identity authentication. For additional information on managed identity authentication method, see [Managed Identity Authentication Overview](../../api/connection-strings/storage-authentication-methods.md#managed-identity).

```kusto
.ingest into table T ('https://StorageAccount.blob.core.windows.net/Container/file.csv;managed_identity=802bada6-4d21-44b2-9d15-e66b29e4d63e')
```

### Azure Data Lake Storage Gen 2

The following example is for ingesting data from Azure Data Lake Storage Gen 2
(ADLSv2). The credentials used here (`...`) are the storage account credentials
(shared key), and we use string obfuscation only for the secret part of the
connection string.

```kusto
.ingest into table T (
  'abfss://myfilesystem@contoso.dfs.core.windows.net/path/to/file1.csv;...'
)
```

### Azure Data Lake Storage

The following example ingests a single file from Azure Data Lake Storage (ADLS).
It uses the user's credentials to access ADLS (so there's no need to treat
the storage URI as containing a secret). It also shows how to specify ingestion
properties.

```kusto
.ingest into table T ('adl://contoso.azuredatalakestore.net/Path/To/File/file1.ext;impersonate')
  with (format='csv')
```

### Amazon S3 with an access key

The following example ingests a single file from Amazon S3 using an [access key ID and a secret access key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys).

```kusto
.ingest into table T ('https://bucketname.s3.us-east-1.amazonaws.com/path/to/file.csv;AwsCredentials=AKIAIOSFODNN7EXAMPLE,wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')
  with (format='csv')
```

### Amazon S3 with a presigned URL

The following example ingests a single file from Amazon S3 using a [preSigned URL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html).

```kusto
.ingest into table T ('https://bucketname.s3.us-east-1.amazonaws.com/file.csv?<<pre signed string>>')
  with (format='csv')
```
